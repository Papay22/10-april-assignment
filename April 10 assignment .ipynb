{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf968b-ef79-4d23-913f-ee6c25e99c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can use Bayes' theorem to calculate the probability of an employee being a smoker given that he/she uses the health insurance plan:\n",
    "\n",
    "\n",
    "P(smoker|uses insurance) = P(uses insurance|smoker) * P(smoker) / P(uses insurance)\n",
    "\n",
    "\n",
    "We are given that 70% of the employees use the health insurance plan, so:\n",
    "\n",
    "\n",
    "P(uses insurance) = 0.7\n",
    "\n",
    "\n",
    "We are also given that 40% of the employees who use the plan are smokers, so:\n",
    "\n",
    "\n",
    "P(uses insurance|smoker) = 0.4\n",
    "\n",
    "\n",
    "Finally, we need to know the prior probability of an employee being a smoker, which is not given in the problem statement. Let's assume that 20% of the employees are smokers:\n",
    "\n",
    "\n",
    "P(smoker) = 0.2\n",
    "\n",
    "\n",
    "Now we can substitute these values into Bayes' theorem:\n",
    "\n",
    "\n",
    "P(smoker|uses insurance) = 0.4 * 0.2 / 0.7\n",
    "\n",
    "= 0.114\n",
    "\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.114 or 11.4%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a78899-5e82-4748-83f1-7d6d1407a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "We are given:\n",
    "\n",
    "\n",
    "P(H) = 0.7, the probability that an employee uses the health insurance plan.\n",
    "P(S|H) = 0.4, the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "We can use Bayes' theorem to calculate P(S|H):\n",
    "\n",
    "\n",
    "P(S|H) = P(H|S) * P(S) / P(H)\n",
    "\n",
    "\n",
    "We can calculate P(H|S) using Bayes' theorem as well:\n",
    "\n",
    "\n",
    "P(H|S) = P(S|H) * P(H) / P(S)\n",
    "\n",
    "\n",
    "We need to calculate P(S), the marginal probability of an employee being a smoker, which we can obtain using the law of total probability:\n",
    "\n",
    "\n",
    "P(S) = P(S|H) * P(H) + P(S|not H) * P(not H)\n",
    "\n",
    "\n",
    "We can assume that the complement of H (not H) is the event that an employee does not use the health insurance plan. Since we are not given any information about the probability of an employee being a smoker and not using the plan, we cannot calculate this term and we will have to make an assumption. Let's assume that the proportion of smokers is the same among employees who use the plan and those who do not. Then:\n",
    "\n",
    "\n",
    "P(S) = P(S|H) * P(H) + P(S|not H) * (1 - P(H))\n",
    "\n",
    "= 0.4 * 0.7 + 0.4 * 0.3\n",
    "\n",
    "= 0.28 + 0.12\n",
    "\n",
    "= 0.4\n",
    "\n",
    "\n",
    "Now we can substitute all these values into Bayes' theorem to get:\n",
    "\n",
    "\n",
    "P(H|S) = P(S|H) * P(H) / P(S)\n",
    "\n",
    "= 0.4 * 0.7 / 0.4\n",
    "\n",
    "= 0.7\n",
    "\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.7 or 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b3e79f-0fe6-4abd-9c91-7d69250b42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of the Naive Bayes algorithm used for classification problems.\n",
    "\n",
    "\n",
    "The main difference between them is the type of data they are designed to work with. Bernoulli Naive Bayes is used when the input features are binary (i.e., they take on values of 0 or 1), while Multinomial Naive Bayes is used when the input features represent counts or frequencies of discrete data (e.g., word counts in a document).\n",
    "\n",
    "\n",
    "In Bernoulli Naive Bayes, the probability of each feature given the class is modeled using a Bernoulli distribution, which assumes that each feature is independent and has a binary outcome. This makes it suitable for text classification tasks where we want to determine whether a document belongs to a particular category based on the presence or absence of certain words.\n",
    "\n",
    "\n",
    "In Multinomial Naive Bayes, the probability of each feature given the class is modeled using a Multinomial distribution, which assumes that each feature represents a count or frequency of a particular event. This makes it suitable for tasks such as sentiment analysis, where we want to determine the sentiment of a piece of text based on the frequency of certain words.\n",
    "\n",
    "\n",
    "Overall, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the data and the specific requirements of the classification task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf5383-a17f-4164-b4a6-953b145bddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification problems. In multi-class classification, the goal is to predict the class label of an instance from a set of possible class labels. Gaussian Naive Bayes is a probabilistic algorithm that calculates the probability of an instance belonging to each class label and selects the label with the highest probability as the predicted label.\n",
    "\n",
    "\n",
    "In the case of multi-class classification, Gaussian Naive Bayes can be extended to handle multiple classes by using a one-vs-all (OvA) approach. In this approach, we train a separate binary classifier for each class label, where the positive class is the target class and the negative class is all other classes combined. During prediction, we apply each binary classifier to the instance and select the class label with the highest probability.\n",
    "\n",
    "\n",
    "Alternatively, Gaussian Naive Bayes can also be extended to handle multiple classes using a one-vs-one (OvO) approach. In this approach, we train a separate binary classifier for each pair of class labels and use a voting scheme to determine the final predicted label. While OvA is more computationally efficient, OvO can be more accurate in some cases.\n",
    "\n",
    "\n",
    "Overall, Gaussian Naive Bayes can be a useful algorithm for multi-class classification problems, particularly when the input features are continuous and follow a Gaussian distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
